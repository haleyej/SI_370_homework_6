{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# SI 370 - Homework 6 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 2022.11.16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you'll apply your knowledge of classification to text analysis, specifically real and fake news. Your task is to predict whether a news article is real or fake using the available information.\n",
    "\n",
    "The dataset that you'll use is described at https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset\n",
    "as well as the following references:\n",
    "\n",
    "Ahmed H, Traore I, Saad S. “Detecting opinion spams and fake news using text classification”, Journal of Security and Privacy, Volume 1, Issue 1, Wiley, January/February 2018.\n",
    "\n",
    "Ahmed H, Traore I, Saad S. (2017) “Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques. In: Traore I., Woungang I., Awad A. (eds) Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments. ISDDC 2017. Lecture Notes in Computer Science, vol 10618. Springer, Cham (pp. 127-138).\n",
    "\n",
    "You will probably get the most informative information from the content of the articles as well as their titles.  \n",
    "\n",
    "You have at your disposal several\n",
    "techniques that you can use to create features from text, including, word embedding, part-of-speech analysis (from SI 330), and so on.  You might want to use CountVectorizer and/or TfidfVectorizer from the\n",
    "sklearn.feature_extraction library, which are described below.\n",
    "\n",
    "You should pre-process your text using at least some of the steps outlined in lectures (e.g. normalizing to lowercase, splitting into words, etc.).\n",
    "\n",
    "The articles are provided in two different files: Fake.csv and True.csv.  We recommend that you create a dataframe with the contents of those files combined, including a new column that specifies whether the article is real or fake (note that you can use whatever coding you want for \"real\" vs. \"fake\", e.g. 1 and 0, \"real\" and \"fake\", \"false\" and \"true\" -- whatever works for you.\n",
    "\n",
    "You should split the resulting combined dataframe into training and testing datasets OR use cross-validation.  If you go the splitting-into-training-and-testing route, we recommend an 80-20 split (i.e. training gets 80% of the data; testing gets 20%) and use the testing dataset to report your accuracy score.  If you go the cross-validation route, we recommend using 5-fold cross-validation and use the mean accuracy score for your 5 folds when reporting your accuracy score.\n",
    "\n",
    "\n",
    "Much like the previous homework assignment, you'll want to try a variety of classifiers and possibly use an ensemble.  And, in a similar way to the previous homework assignment, your submission (to Canvas -- there is no requirement to submit this anywhere else, including Kaggle) should be based on a Jupyter notebook that you create.\n",
    "\n",
    "Points will be allocated as follows:\n",
    "\n",
    "| Component | Points |\n",
    "|:---|:---|\n",
    "|1. Text pre-processing and feature extraction, including justification for your choices| 8 |\n",
    "|2. Use of at least three classifiers, not including VotingClassifier (if you use it) |  6  |\n",
    "|3. Accuracy (based on test dataset)| 75%: 2 , 80%: 4 , 90%: 6  |\n",
    "\n",
    "Note that you are welcome to use VotingClassifier to improve your accuracy, you just can't count it as one of the three classifiers for points in Component 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following tutorial is from https://www.kaggle.com/adamschroeder/countvectorizer-tfidfvectorizer-predict-comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "d185e5fb-ac9a-40a2-b563-d9aa1a77f94e",
    "_uuid": "1008fd2001c2b8485d2b4815813f90b722286c22"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk import Text\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e410d5e3-a909-4dde-9d13-d1963063469b",
    "_uuid": "5b534ccd8f7584c1e97adf86486664fb0450af58"
   },
   "source": [
    "# CountVectorizer -- Brief Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b7098871-bb8d-4460-9019-4fc61b4ebd33",
    "_uuid": "a7a2bd7c30e900fb6ad66091868a647670626490"
   },
   "source": [
    "*    CountVectorizer can lowercase letters, disregard punctuation and stopwords, but it can't LEMMATIZE or STEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "0394cd12-20e6-4143-bf06-c69b2bfe1406",
    "_uuid": "376ae829a98e2b2de88e51e099e71d5e0b10e190"
   },
   "outputs": [],
   "source": [
    "# Stopping by Woods on a Snowy Evening by Robert Frost\n",
    "txt = [\"\"\"Whose woods these are I think I know.\n",
    "His house is in the village though;\n",
    "He will not see me stopping here\n",
    "To watch his woods fill up with snow.\"\"\",\n",
    "       \n",
    "\"\"\"My little horse must think it queer\n",
    "To stop without a farmhouse near\n",
    "Between the woods and frozen lake\n",
    "The darkest evening of the year.\"\"\",\n",
    "\n",
    "\"\"\"He gives his harness bells a shake\n",
    "To ask if there is some mistake.\n",
    "The only other sound's the sweep\n",
    "Of easy wind and downy flake.\"\"\",\n",
    "\n",
    "\"\"\"The woods are lovely, dark and deep,\n",
    "But I have promises to keep,\n",
    "And miles to go before I sleep,\n",
    "And miles to go before I sleep.\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Whose woods these are I think I know.\\nHis house is in the village though;\\nHe will not see me stopping here\\nTo watch his woods fill up with snow.',\n",
       " 'My little horse must think it queer\\nTo stop without a farmhouse near\\nBetween the woods and frozen lake\\nThe darkest evening of the year.',\n",
       " \"He gives his harness bells a shake\\nTo ask if there is some mistake.\\nThe only other sound's the sweep\\nOf easy wind and downy flake.\",\n",
       " 'The woods are lovely, dark and deep,\\nBut I have promises to keep,\\nAnd miles to go before I sleep,\\nAnd miles to go before I sleep.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b71fb093-b9ef-47e4-9e82-28cb6eaad710",
    "_uuid": "b6481fd89ec95f6766aa3895d2a6fe89686dce5b"
   },
   "source": [
    "**Features in Bag of Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "0fad19fd-db7f-4d75-bb72-93d9cea919a1",
    "_uuid": "0ae95c0f33c248326acea22e4f908d011e85124c",
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every feature:\n",
      "['ask', 'bells', 'dark', 'darkest', 'deep', 'downy', 'easy', 'evening', 'farmhouse', 'flake', 'frozen', 'gives', 'harness', 'horse', 'house', 'know', 'lake', 'little', 'lovely', 'miles', 'mistake', 'near', 'promises', 'queer', 'shake', 'sleep', 'snow', 'sound', 'stop', 'stopping', 'sweep', 'think', 'village', 'watch', 'wind', 'woods', 'year']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haleyjohnson/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Initialize a CountVectorizer object: count_vectorizer\n",
    "count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None)\n",
    "\n",
    "# Transforms the data into a bag of words\n",
    "count_train = count_vec.fit(txt)\n",
    "bag_of_words = count_vec.transform(txt)\n",
    "\n",
    "# Print the first 10 features of the count_vec\n",
    "print(\"Every feature:\\n{}\".format(count_vec.get_feature_names()))\n",
    "# print(\"\\nEvery 3rd feature:\\n{}\".format(count_vec.get_feature_names()[::3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "950bcc7b-4011-4a65-af4c-ca46e0ff1bc7",
    "_uuid": "e2e7d7569b95d531fa3960d0d452690a1e2e6151"
   },
   "source": [
    "**Vocabulary and vocabulary ID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "730a0d62-b034-4a78-9e5a-906e0fefffa2",
    "_uuid": "ed2b7bc16a0c04c33f946661904dc95018c49235"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 37\n",
      "Vocabulary content:\n",
      " {'woods': 35, 'think': 31, 'know': 15, 'house': 14, 'village': 32, 'stopping': 29, 'watch': 33, 'snow': 26, 'little': 17, 'horse': 13, 'queer': 23, 'stop': 28, 'farmhouse': 8, 'near': 21, 'frozen': 10, 'lake': 16, 'darkest': 3, 'evening': 7, 'year': 36, 'gives': 11, 'harness': 12, 'bells': 1, 'shake': 24, 'ask': 0, 'mistake': 20, 'sound': 27, 'sweep': 30, 'easy': 6, 'wind': 34, 'downy': 5, 'flake': 9, 'lovely': 18, 'dark': 2, 'deep': 4, 'promises': 22, 'miles': 19, 'sleep': 25}\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size: {}\".format(len(count_train.vocabulary_)))\n",
    "print(\"Vocabulary content:\\n {}\".format(count_train.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 2, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1],\n",
       "       [1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0,\n",
       "        1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f4a4ad33-2cc2-4f38-b8f5-66c4cf9e449c",
    "_uuid": "ceaed2e0a255f2ab235b9694eb5115aaa7eb0796"
   },
   "source": [
    "# N-grams (sets of consecutive words)\n",
    "* N=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "3e233e9b-cd56-4f86-a977-e0fcb034b61d",
    "_uuid": "9aebf4daeec289943a79def5f7d1b777e495d606"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ask', 'ask mistake', 'bells', 'bells shake', 'dark', 'dark deep', 'darkest', 'darkest evening', 'deep', 'deep promises', 'downy', 'downy flake', 'easy', 'easy wind', 'evening', 'evening year', 'farmhouse', 'farmhouse near', 'flake', 'frozen', 'frozen lake', 'gives', 'gives harness', 'harness', 'harness bells', 'horse', 'horse think', 'house', 'house village', 'know', 'know house', 'lake', 'lake darkest', 'little', 'little horse', 'lovely', 'lovely dark', 'miles', 'miles sleep', 'mistake', 'mistake sound', 'near', 'near woods', 'promises', 'promises miles', 'queer', 'queer stop', 'shake', 'shake ask', 'sleep', 'sleep miles', 'snow', 'sound', 'sound sweep', 'stop', 'stop farmhouse', 'stopping', 'stopping watch', 'sweep', 'sweep easy', 'think', 'think know', 'think queer', 'village', 'village stopping', 'watch', 'watch woods', 'wind', 'wind downy', 'woods', 'woods frozen', 'woods lovely', 'woods snow', 'woods think', 'year']\n"
     ]
    }
   ],
   "source": [
    "count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 2), max_df=1.0, min_df=1, max_features=None)\n",
    "\n",
    "count_train = count_vec.fit(txt)\n",
    "bag_of_words = count_vec.transform(txt)\n",
    "\n",
    "print(count_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e39dc6fb-de14-4c5d-8768-8b0ebdce6a75",
    "_uuid": "6dcc6ce87193525b686a588019d36e2411e26a3a"
   },
   "source": [
    "* N=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "6d6d655b-b3d2-4a28-a435-cdf46c601cc5",
    "_uuid": "0d37a520725d1ee77cf57e251c3d80f0efa59ffa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ask', 'ask mistake', 'ask mistake sound', 'bells', 'bells shake', 'bells shake ask', 'dark', 'dark deep', 'dark deep promises', 'darkest', 'darkest evening', 'darkest evening year', 'deep', 'deep promises', 'deep promises miles', 'downy', 'downy flake', 'easy', 'easy wind', 'easy wind downy', 'evening', 'evening year', 'farmhouse', 'farmhouse near', 'farmhouse near woods', 'flake', 'frozen', 'frozen lake', 'frozen lake darkest', 'gives', 'gives harness', 'gives harness bells', 'harness', 'harness bells', 'harness bells shake', 'horse', 'horse think', 'horse think queer', 'house', 'house village', 'house village stopping', 'know', 'know house', 'know house village', 'lake', 'lake darkest', 'lake darkest evening', 'little', 'little horse', 'little horse think', 'lovely', 'lovely dark', 'lovely dark deep', 'miles', 'miles sleep', 'miles sleep miles', 'mistake', 'mistake sound', 'mistake sound sweep', 'near', 'near woods', 'near woods frozen', 'promises', 'promises miles', 'promises miles sleep', 'queer', 'queer stop', 'queer stop farmhouse', 'shake', 'shake ask', 'shake ask mistake', 'sleep', 'sleep miles', 'sleep miles sleep', 'snow', 'sound', 'sound sweep', 'sound sweep easy', 'stop', 'stop farmhouse', 'stop farmhouse near', 'stopping', 'stopping watch', 'stopping watch woods', 'sweep', 'sweep easy', 'sweep easy wind', 'think', 'think know', 'think know house', 'think queer', 'think queer stop', 'village', 'village stopping', 'village stopping watch', 'watch', 'watch woods', 'watch woods snow', 'wind', 'wind downy', 'wind downy flake', 'woods', 'woods frozen', 'woods frozen lake', 'woods lovely', 'woods lovely dark', 'woods snow', 'woods think', 'woods think know', 'year']\n"
     ]
    }
   ],
   "source": [
    "count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 3), max_df=1.0, min_df=1, max_features=None)\n",
    "\n",
    "count_train = count_vec.fit(txt)\n",
    "bag_of_words = count_vec.transform(txt)\n",
    "\n",
    "print(count_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x110 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 113 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 0, 0, 0, 0, 1, 1, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "        0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "772bca4c-7f5e-4002-92b3-3778af49e7b4",
    "_uuid": "3b591bc9413f1afb857620edb6cd6752b043f5d9"
   },
   "source": [
    "# Min_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "175e2d4f-57a2-413d-9475-38a8cff0c9c9",
    "_uuid": "a5b3098a5250f15b5ab46ad69394155a665853bf"
   },
   "source": [
    "**Min_df** ignores terms that have a document frequency (presence in % of documents) strictly lower than the given threshold. For example, Min_df=0.66 requires that a term appear in 66% of the docuemnts for it to be considered part of the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "04bc030a-2065-44c9-b650-79d592ce1ff8",
    "_uuid": "e2cd00e3929caa6fbbd500d3ecbd29dc4966100a"
   },
   "source": [
    "**Sometimes min_df is used to limit the vocabulary size, so it learns only those terms that appear in at least 10%, 20%, etc. of the documents.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dc5dec8d-7c0c-4a1c-995c-12d47521ddad",
    "_uuid": "713d59da4bdb79c32047bd83758effcb2c21bb57"
   },
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 1), max_df=1.0, min_df=0.75, max_features=None)\n",
    "\n",
    "count_train = count_vec.fit(txt)\n",
    "bag_of_words = count_vec.transform(txt)\n",
    "\n",
    "print(count_vec.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only \"woods\" appears in 3 out of 4 (75%) of the \"documents\" (stanzas), which is why other words are not included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c585fd8c-c8d7-4135-8631-2cab061d2a21",
    "_uuid": "e50a5e259f7d1a41455c78454f94c2490852278e"
   },
   "source": [
    "# Max_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cd77ec37-22bc-4c7c-b104-be9cdbc33b15",
    "_uuid": "2555d4de9fce90b612402b09bf994359297b95e0"
   },
   "source": [
    "When building the vocabulary, it ignores terms that have a document frequency strictly higher than the given threshold. This could be used to exclude terms that are too frequent and are unlikely to help predict the label. For example, by analyzing reviews on the movie Lion King, the term 'Lion' might appear in 90% of the reviews (documents), in which case, we could consider establishing Max_df=0.89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bce42711-e450-406c-a863-4c50f0eb41bb",
    "_uuid": "f0916d8143e9d6538b87fb81c6d1b9c80452cb0b"
   },
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 1), max_df=0.74, min_df=1, max_features=None)\n",
    "\n",
    "count_train = count_vec.fit(txt)\n",
    "bag_of_words = count_vec.transform(txt)\n",
    "\n",
    "print(count_vec.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only \"woods\", in addition to the stopwords, is left out because it occurs in more than 74% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bba8aa26-d2c1-442e-9ecb-7f4b54bb8c96",
    "_uuid": "59a326fbe009ef970268b6432b5541f661703ee1"
   },
   "source": [
    "# Max_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4486c0c4-e316-4074-9c3c-05fbac40daac",
    "_uuid": "ad23dfaed3f75e0bce22669ceef43623a0c0825e"
   },
   "source": [
    "Limit the amount of features (vocabulary) that the vectorizer will learn, based on highest term frequency, although see TfiftVectorizer below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "65869d10-40fd-435d-bc5f-cc599208f37c",
    "_uuid": "3e376fb48a8fa72be1ccf76a48d52bcd593056fc"
   },
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', \n",
    "                            ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=4)\n",
    "\n",
    "count_train = count_vec.fit(txt)\n",
    "bag_of_words = count_vec.transform(txt)\n",
    "\n",
    "print(count_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9d85e6ea-2f25-4f14-8cf5-0c454999f911",
    "_uuid": "2a7123450a186ae74521da4da847161fd92e9af3"
   },
   "source": [
    "# TfidfVectorizer -- Brief Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bbb398f7-2593-4412-9545-7db93628d334",
    "_uuid": "62c7c509333505e0173deb4ab7d27738cf940023"
   },
   "source": [
    "The goal of using tf-idf is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus. (https://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/feature_extraction/text.py#L1365)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1176ec41-b021-4524-9936-7c6c56e13f7b",
    "_uuid": "0cf8e9c74cebae1be6ce69b13fe0224c636155f4"
   },
   "source": [
    "formula used: \n",
    "$tfidf(d, t) = tf(t) \\times idf(d, t)$\n",
    "\n",
    "* tf(t)= the term frequency is the number of times the term appears in the document\n",
    "* idf(d, t) = the document frequency is the number of documents 'd' that contain term 't'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fd0b0b13-ab84-4771-a99d-74bd9819e7db",
    "_uuid": "7f5155fcce0056330a680368dea2161570783e1d"
   },
   "outputs": [],
   "source": [
    "txt1 = ['His smile was not perfect', 'His smile was not not not not perfect', 'She did not sing']\n",
    "tf = TfidfVectorizer(smooth_idf=False, sublinear_tf=False, norm=None, analyzer='word')\n",
    "txt_fitted = tf.fit(txt1)\n",
    "txt_transformed = txt_fitted.transform(txt1)\n",
    "print(\"The text: \", txt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "25f000cf-02e1-4bb8-af0d-6c88828ab9f7",
    "_uuid": "279a982ce89664fcb87b6bb79bbc82c743d0deae"
   },
   "source": [
    "The learned corpus vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "35c2e277-6d95-4ae9-b338-d471053ba887",
    "_uuid": "9168c7438a90a71b0e8e988761682c5c0f05f10b"
   },
   "outputs": [],
   "source": [
    "tf.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "73e8356b-b2e1-4630-b8f7-cc4910d99dae",
    "_uuid": "d0b14b583014041ed8c04aee71737e4bbf47ffab"
   },
   "source": [
    "**IDF:** The inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c3f5b1b9-29d8-451b-988e-c913f725c5ef",
    "_uuid": "ea54e8f36de0ba1d33670ea1b25f6e908fc55e48",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idf = tf.idf_\n",
    "print(dict(zip(txt_fitted.get_feature_names(), idf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the tokens 'sang','she' have the most idf weight because \n",
    "they are the only tokens that appear in one document only.\n",
    "The token 'not' appears 6 times but it is also in all documents, so its idf is the lowest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8170cef9-c18c-419a-9549-97f05e0cba6d",
    "_uuid": "3875bc2d774af0b145dae24d49cddce41061794e"
   },
   "source": [
    "Graphing inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a688eebb-616e-447c-981e-8b4710355b55",
    "_uuid": "90107fb2986dca9bcfa54a117ce6e5f70175c156"
   },
   "outputs": [],
   "source": [
    "rr = dict(zip(txt_fitted.get_feature_names(), idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5631f6f6-0e10-4ce0-8d41-bd2a740742cf",
    "_uuid": "2841ce8e683e2dc4995db669a1b24f0af851c074",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token_weight = pd.DataFrame.from_dict(rr, orient='index').reset_index()\n",
    "token_weight.columns = ('token', 'weight')\n",
    "token_weight = token_weight.sort_values(by='weight', ascending=False)\n",
    "token_weight\n",
    "\n",
    "sns.barplot(x='token', y='weight', data=token_weight)            \n",
    "plt.title(\"Inverse Document Frequency(idf) per token\")\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1780ee0-f805-4125-8854-adce357e4914",
    "_uuid": "e8de64851a2aba585aa11b711fd8919763de456b"
   },
   "source": [
    "Listing (instead of graphing) inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5323f75b-0cb6-4880-b049-7fb974c270ab",
    "_uuid": "759cec0a7d85c2367affcccdb7f1c5f2a8b0ee4e"
   },
   "outputs": [],
   "source": [
    "# get feature names\n",
    "feature_names = np.array(tf.get_feature_names())\n",
    "sorted_by_idf = np.argsort(tf.idf_)\n",
    "print(\"Features with lowest idf:\\n{}\".format(\n",
    "       feature_names[sorted_by_idf[:3]]))\n",
    "print(\"\\nFeatures with highest idf:\\n{}\".format(\n",
    "       feature_names[sorted_by_idf[-3:]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1f34a935-2c22-4500-a551-97a810072426",
    "_uuid": "40bf8ce1fc5d50951716284de9c7cb5783dd8f8b"
   },
   "source": [
    "**Weight of tokens per document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b2432306-2012-478e-b74a-ed80eebc7761",
    "_uuid": "253e04bf9807ea477b703a37205aa8a4609b73a7"
   },
   "outputs": [],
   "source": [
    "print(\"The token 'not' has  the largest weight in document #2 because it appears 3 times there. But in document #1\\\n",
    " its weight is 0 because it does not appear there.\")\n",
    "txt_transformed.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "344187ac-114d-4629-b5a2-b7445edaccc8",
    "_uuid": "540fe13829199d236ce703b6b8039f29497792e6"
   },
   "source": [
    "**Summary:** the more times a token appears in a document, the more weight it will have. However, the more documents the token appears in, it is 'penalized' and the weight is diminished. For example, the weight for token 'not' is 4, but if it did not appear in all documents (that is, only in one document) its weight would have been 8.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f080fbb1-5367-4acb-9e1c-ab5b78414823",
    "_uuid": "3c24d989a7459b6287f685a88c076f2d2fa5955f"
   },
   "source": [
    "**TF-IDF** - Maximum token value throughout the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a8fdc906-7b4c-4c61-a1ad-96bb006fa464",
    "_uuid": "5e4a087db6f509055227304ebf3e021f35b4496d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new1 = tf.transform(txt1)\n",
    "\n",
    "# find maximum value for each of the features over all of dataset:\n",
    "max_val = new1.max(axis=0).toarray().ravel()\n",
    "\n",
    "#sort weights from smallest to biggest and extract their indices \n",
    "sort_by_tfidf = max_val.argsort()\n",
    "\n",
    "print(\"Features with lowest tfidf:\\n{}\".format(\n",
    "      feature_names[sort_by_tfidf[:3]]))\n",
    "\n",
    "print(\"\\nFeatures with highest tfidf: \\n{}\".format(\n",
    "      feature_names[sort_by_tfidf[-3:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(new1.toarray(),columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_code_all_hidden": true,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
